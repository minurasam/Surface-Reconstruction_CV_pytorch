{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minurasam/Surface-Reconstruction_CV_pytorch/blob/main/CSC492592_Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Programming Assignment 3: Object Detection Model Experimentation\n",
        "\n",
        "In this programming assignment, you are tasked with conducting experiments on object detection models, focusing on the differences between single-stage (**YOLO-family**) and two-stage (**RCNN-family**) approaches. Please note that the emphasis of this assignment is on **experimentation and analysis**, rather than on programming itself.\n",
        "\n",
        "You will use the [COCO](https://cocodataset.org/#home) dataset as a public testbench to analyze their performance across various metrics. This assignment provides an opportunity to learn how to evaluate deep learning-based object detection models and understand how to appropriately select a model for potential future applications.\n",
        "\n",
        "***Please run the prepared codes below to answer the following questions:***\n",
        "\n",
        "- [**50 pts**] Using a `sample size` of **400**, compare the performance of each model in terms of the following metrics: `mAP`, `f1-score`, `confusion matrix`, and `execution time` (inference speed per second).\n",
        " using plots and provide your interpretation on the results with your own opinion.\n",
        "- [**25 pts**] For each model, evaluate its performance across different sample sizes: [**50,100,200,400**], focusing on metrics such as `mAP`, `F1-score`, and the `confusion matrix`. Please provide your interpretation of the results, especially from the perspective of **class balance**. Discuss how variations in sample size might affect model performance for different classes and share your insights on this matter.\n",
        "- [**25 pts**] Please write a short paragraph describing your opinion on usecase scenarios or applications where a **two-stage** model might be preferred over a **single-stage** model. Consider factors such as real-time processing requirements, accuracy needs, computational resources, and the specific characteristics of the task or dataset.\n",
        "\n",
        "***Submission:***\n",
        "- Ensure your report is well-organized, with clear sections for each task.\n",
        "- Ensure that your analysis reflects your own interpretation, rather than merely comparing numerical values.\n",
        "- Submit your report in PDF format to D2L by ***April 10, 2024***.\n",
        "\n",
        "\n",
        "***Additional Resources:***\n",
        "- [YoloV5: Tutorial by ultralytics](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb#scrollTo=1NcFxRcFdJ_O)\n",
        "- [Faster-RCNN: Tutorial by PyTorch](https://pytorch.org/vision/master/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html)\n",
        "- [The COCO Dataset: Best Practices for Downloading, Visualization, and Evaluation](https://medium.com/voxel51/the-coco-dataset-best-practices-for-downloading-visualization-and-evaluation-68a3d7e97fb7)"
      ],
      "metadata": {
        "id": "aiRdY0IxcUuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "id": "EtuQWEUKeqwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install package\n",
        "!pip install fiftyone"
      ],
      "metadata": {
        "id": "-bDs6uDhM1Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from fiftyone import ViewField as F\n",
        "\n",
        "# Load coco-2017 validation dataset\n",
        "######################\n",
        "# TODO: Sample items #\n",
        "######################\n",
        "dataset = foz.load_zoo_dataset(\"coco-2017\",\n",
        "                               split=\"validation\",\n",
        "                               label_types=[\"detections\"],\n",
        "                               shuffle=True,\n",
        "                               max_samples=400,\n",
        "                               )\n",
        "\n",
        "# Load model from zoo and apply it to dataset\n",
        "######################\n",
        "# TODO: Load a model #\n",
        "######################\n",
        "model1 = foz.load_zoo_model(\"yolov5n-coco-torch\") # Model size: 7.76 KB\n",
        "model2 = foz.load_zoo_model(\"yolov8n-coco-torch\") # Model size: 6.23 MB\n",
        "model3 = foz.load_zoo_model(\"faster-rcnn-resnet50-fpn-coco-torch\") # Model size: 207.71 MB\n",
        "# model4 = foz.load_zoo_model(\"mask-rcnn-resnet50-fpn-coco-torch\")   # Model size: 169.84 MB"
      ],
      "metadata": {
        "id": "bMjoN_oYPg8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = fo.launch_app(dataset)"
      ],
      "metadata": {
        "id": "FQNVxd1R5AXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/root/\")"
      ],
      "metadata": {
        "id": "F-Z79OWQXyeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow==7"
      ],
      "metadata": {
        "id": "Mb8hYjYRbKja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Size 400 Model 1"
      ],
      "metadata": {
        "id": "1AMJ2zLLAZm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.apply_model(model2, label_field=\"predictions\")\n",
        "\n",
        "# Evaluate `predictions` w.r.t. labels in `ground_truth` field\n",
        "######################################################################\n",
        "# TODO: Evaluate the model with IoU (Intersection over Union) of 0.5 #\n",
        "######################################################################\n",
        "results = dataset.evaluate_detections(\n",
        "    \"predictions\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"eval\",\n",
        "    compute_mAP=True,\n",
        "    iou=0.5,\n",
        ")\n",
        "\n",
        "\n",
        "#########################################################\n",
        "# TODO: Analyze COCO mAP (mean Average Precision) score #\n",
        "#########################################################\n",
        "mapscore = results.mAP()\n",
        "print(f\"mAP: {mapscore}\")\n",
        "\n",
        "\n",
        "# Print a classification report for the top-10 classes\n",
        "results.print_report()\n",
        "\n",
        "# Plot a confusion matrix\n",
        "plot = results.plot_confusion_matrix()\n",
        "plot.show()"
      ],
      "metadata": {
        "id": "VJIzgQ9yvFIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"person\", \"kite\", \"car\", \"bird\", \"carrot\", \"chair\", \"bowl\", \"bottle\", \"book\", \"cup\", \"dining table\", \"umbrella\"]\n",
        "\n",
        "# Print a classification report for the top-10 classes\n",
        "results.print_report(classes=classes)\n",
        "\n",
        "# Print some statistics about the total TP/FP/FN counts\n",
        "print(\"TP: %d\" % dataset.sum(\"eval_tp\"))\n",
        "print(\"FP: %d\" % dataset.sum(\"eval_fp\"))\n",
        "print(\"FN: %d\" % dataset.sum(\"eval_fn\"))\n",
        "\n",
        "print(\"FN: %d\" % dataset.sum(\"eval_fn\"))\n",
        "\n",
        "# Create a view that has samples with the most false positives first, and\n",
        "# only includes false positive boxes in the `predictions` field\n",
        "view = (\n",
        "    dataset\n",
        "    .sort_by(\"eval_fp\", reverse=True)\n",
        "    .filter_labels(\"predictions\", F(\"eval\") == \"fp\")\n",
        ")\n",
        "\n",
        "plot = results.plot_confusion_matrix(classes=classes)\n",
        "plot.show()\n",
        "\n",
        "# Plot precision-recall curve\n",
        "plot2 = results.plot_pr_curves(classes=classes)\n",
        "plot2.show()"
      ],
      "metadata": {
        "id": "ULcRGkoIsvdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.apply_model(model1, label_field=\"predictions\")\n",
        "\n",
        "# Evaluate `predictions` w.r.t. labels in `ground_truth` field\n",
        "######################################################################\n",
        "# TODO: Evaluate the model with IoU (Intersection over Union) of 0.5 #\n",
        "######################################################################\n",
        "results = dataset.evaluate_detections(\n",
        "    \"predictions\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"eval\",\n",
        "    compute_mAP=True,\n",
        "    iou=0.5,\n",
        ")\n",
        "\n",
        "\n",
        "#########################################################\n",
        "# TODO: Analyze COCO mAP (mean Average Precision) score #\n",
        "#########################################################\n",
        "mapscore = results.mAP()\n",
        "print(f\"mAP: {mapscore}\")\n",
        "\n",
        "\n",
        "# Print a classification report for the top-10 classes\n",
        "results.print_report()\n",
        "\n",
        "# Plot a confusion matrix\n",
        "plot = results.plot_confusion_matrix()\n",
        "plot.show()"
      ],
      "metadata": {
        "id": "cXkNnur2oyxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"person\", \"kite\", \"car\", \"bird\", \"carrot\", \"chair\", \"bowl\", \"bottle\", \"book\", \"cup\", \"dining table\", \"umbrella\"]\n",
        "\n",
        "# Print a classification report for the top-10 classes\n",
        "results.print_report(classes=classes)\n",
        "\n",
        "# Print some statistics about the total TP/FP/FN counts\n",
        "print(\"TP: %d\" % dataset.sum(\"eval_tp\"))\n",
        "print(\"FP: %d\" % dataset.sum(\"eval_fp\"))\n",
        "print(\"FN: %d\" % dataset.sum(\"eval_fn\"))\n",
        "\n",
        "print(\"FN: %d\" % dataset.sum(\"eval_fn\"))\n",
        "\n",
        "# Create a view that has samples with the most false positives first, and\n",
        "# only includes false positive boxes in the `predictions` field\n",
        "view = (\n",
        "    dataset\n",
        "    .sort_by(\"eval_fp\", reverse=True)\n",
        "    .filter_labels(\"predictions\", F(\"eval\") == \"fp\")\n",
        ")\n",
        "\n",
        "plot = results.plot_confusion_matrix(classes=classes)\n",
        "plot.show()\n",
        "\n",
        "# Plot precision-recall curve\n",
        "plot2 = results.plot_pr_curves(classes=classes)\n",
        "plot2.show()"
      ],
      "metadata": {
        "id": "xf7qns2Fc8jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot = results.plot_pr_curves(classes=[\"person\", \"kite\", \"car\"])\n",
        "plot.show()"
      ],
      "metadata": {
        "id": "RxJ5pIOTu6sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a view that has samples with the most false positives first, and\n",
        "# only includes false positive boxes in the `predictions` field\n",
        "view = (\n",
        "    dataset\n",
        "    .sort_by(\"eval_fp\", reverse=True)\n",
        "    .filter_labels(\"predictions\", F(\"eval\") == \"fp\")\n",
        ")\n",
        "\n",
        "# Visualize results in the App\n",
        "session = fo.launch_app(view=view)"
      ],
      "metadata": {
        "id": "noSN_yO0jLeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faster RCNN"
      ],
      "metadata": {
        "id": "VLomDbCTuYXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.apply_model(model3, label_field=\"predictions\")\n",
        "\n",
        "# Evaluate `predictions` w.r.t. labels in `ground_truth` field\n",
        "######################################################################\n",
        "# TODO: Evaluate the model with IoU (Intersection over Union) of 0.5 #\n",
        "######################################################################\n",
        "results = dataset.evaluate_detections(\n",
        "    \"predictions\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"eval\",\n",
        "    compute_mAP=True,\n",
        "    iou=0.5,\n",
        ")\n",
        "\n",
        "\n",
        "#########################################################\n",
        "# TODO: Analyze COCO mAP (mean Average Precision) score #\n",
        "#########################################################\n",
        "mapscore = results.mAP()\n",
        "print(f\"mAP: {mapscore}\")\n",
        "\n",
        "\n",
        "# Print a classification report for the top-10 classes\n",
        "results.print_report()\n",
        "\n",
        "# Plot a confusion matrix\n",
        "plot = results.plot_confusion_matrix()\n",
        "plot.show()"
      ],
      "metadata": {
        "id": "_o6PvWIipDVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"person\", \"kite\", \"car\", \"bird\", \"carrot\", \"chair\", \"bowl\", \"bottle\", \"book\", \"cup\", \"dining table\", \"umbrella\"]\n",
        "\n",
        "# Print a classification report for the top-10 classes\n",
        "results.print_report(classes=classes)\n",
        "\n",
        "# Print some statistics about the total TP/FP/FN counts\n",
        "print(\"TP: %d\" % dataset.sum(\"eval_tp\"))\n",
        "print(\"FP: %d\" % dataset.sum(\"eval_fp\"))\n",
        "print(\"FN: %d\" % dataset.sum(\"eval_fn\"))\n",
        "\n",
        "print(\"FN: %d\" % dataset.sum(\"eval_fn\"))\n",
        "\n",
        "# Create a view that has samples with the most false positives first, and\n",
        "# only includes false positive boxes in the `predictions` field\n",
        "view = (\n",
        "    dataset\n",
        "    .sort_by(\"eval_fp\", reverse=True)\n",
        "    .filter_labels(\"predictions\", F(\"eval\") == \"fp\")\n",
        ")\n",
        "\n",
        "plot = results.plot_confusion_matrix(classes=classes)\n",
        "plot.show()\n",
        "\n",
        "# Plot precision-recall curve\n",
        "plot2 = results.plot_pr_curves(classes=classes)\n",
        "plot2.show()"
      ],
      "metadata": {
        "id": "FXrJJHDVufY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ioyxgZUxvwxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}